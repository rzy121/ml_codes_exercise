{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor Exercise\n",
    "\n",
    "* Exercise based on [this article](https://levelup.gitconnected.com/building-a-decision-tree-from-scratch-in-python-machine-learning-from-scratch-part-ii-6e2e56265b19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for decision tree regressor. This makes use of the Node class with will be created later\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    \n",
    "    def fit(self, X, y, min_leaf = 5):\n",
    "        self.dtree = Node(X, y, np.arange(len(y)),min_leaf)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.dtree.predict(X.values)\n",
    "    \n",
    "# create class for node. this class represents one decision point in the tree, and also holds data it splits\n",
    "    \n",
    "class Node:\n",
    "    \n",
    "    def __init__(self, X, y, idx, min_leaf=5):\n",
    "        self.X, self.y, self.idx, self.min_leaf = X, y, idx, min_leaf # idx is the indices for the rows at this node\n",
    "        self.row_cnt = len(idx) # total row count at the node\n",
    "        self.col_cnt = X.shape[1] # total column count at the node\n",
    "        self.val = np.mean(y[idx]) # prediction at this node, which is just mean of all the rows at this node\n",
    "        self.score = float('Inf') # metric that evalutes the purity of the node. variance or std for regression\n",
    "        \n",
    "        # this method finds the point where the data should be split. to be defined next\n",
    "        # this will run recursively until the leaf node is reached\n",
    "        self.find_varsplit()  \n",
    "        \n",
    "    def find_varsplit(self):\n",
    "        \n",
    "        for c in range(self.col_cnt):\n",
    "            # run the find_better_split method for each column in this node\n",
    "            # the method essentially checks which column is the best to split data on\n",
    "            self.find_better_split(c) \n",
    "        \n",
    "        if self.is_leaf: return # quit if the node is already a leaf node. stopping condition for the recursion\n",
    "        X = self.split_col # the columns to be split\n",
    "        \n",
    "        ls = np.nonzero(X <= self.split)[0] # save indices of rows <= the split value, save it to the left side\n",
    "        rs = np.nonzero(X > self.split)[0] # save indices of rows > the split value, save it to the right side\n",
    "        \n",
    "        self.ls = Node(self.X, self.y, self.idx[ls], self.min_leaf) # left side at the node split\n",
    "        self.rs = Node(self.X, self.y, self.idx[rs], self.min_leaf) # right side at the node split\n",
    "\n",
    "    @property\n",
    "    def split_col(self): return self.X.values[self.idx, self.var_idx]\n",
    "    \n",
    "    # check if leaf node is reached\n",
    "    @property\n",
    "    def is_leaf(self): return self.score == float('Inf')\n",
    "        \n",
    "    # define the find_better_split that finds the best column to split on\n",
    "    def find_better_split(self, var_idx):\n",
    "        \n",
    "        X = self.X.values[self.idx, var_idx]\n",
    "        \n",
    "        # within each column, split the rows into 2 groups (left & right) at each value of the column\n",
    "        for r in range(self.row_cnt):\n",
    "            ls = X <= X[r]\n",
    "            rs = X > X[r]\n",
    "            # quit the loop if either of the sub group is smaller than the minimum leaf node size\n",
    "            if rs.sum() < self.min_leaf or ls.sum() < self.min_leaf: continue\n",
    "            \n",
    "            # calcuate the metric for each split using the find_score method (to be defined later)\n",
    "            curr_score = self.find_score(ls, rs)\n",
    "            \n",
    "            # if the current metric is better (smaller) than the previous one:\n",
    "            if curr_score < self.score:\n",
    "                self.var_idx = var_idx # update the current column to be the best column\n",
    "                self.score = curr_score # update the current score to be the best score\n",
    "                self.split = X[r] # update the current column value to be the best split value\n",
    "            \n",
    "    # define the find_score method to calculate the score metric (weighted avg of std in this case)\n",
    "    def find_score(self, ls, rs):\n",
    "        y = self.y[self.idx]\n",
    "        ls_std = y[ls].std()\n",
    "        rs_std = y[rs].std()\n",
    "        \n",
    "        # return weigted avg of stds from both groups\n",
    "        return ls_std * ls.sum() + rs_std * rs.sum()\n",
    "    \n",
    "    # method for prediction, using predict_row method\n",
    "    def predict(self, X):    \n",
    "        return np.array([self.predict_row(xi) for xi in X])\n",
    "    \n",
    "    # recursively compare new data with the best split values, and divide them into left and right sides\n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf: return self.val\n",
    "        node = self.ls if xi[self.var_idx] <= self.split else self.rs\n",
    "        return node.predict_row(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train, test data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['OverallQual', 'GrLivArea', 'GarageCars']]\n",
    "y_train =train['SalePrice']\n",
    "\n",
    "X_test = test[['OverallQual', 'GrLivArea', 'GarageCars']]\n",
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test results\n",
    "regressor = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "preds = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74384.50428979043"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73779.71919388912"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor as skdtr\n",
    "\n",
    "sk_tree = skdtr(min_samples_leaf = 5)\n",
    "sk_tree.fit(X_train, y_train)\n",
    "preds_sk = sk_tree.predict(X_test)\n",
    "preds_sk.std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
