{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise for Back Propagation\n",
    "\n",
    "Following example from [this blog](https://brilliant.org/wiki/backpropagation/)\n",
    "\n",
    "Main steps for the algorithm:\n",
    "1. Calculate the _forward phase_ from the input layer to the final output layer-m, and store the results\n",
    "    - final prediction $\\hat y_{d}$\n",
    "    - activation input (node-j at layer-k) $a_j^k$\n",
    "    - output (node-j at layer-k) $o_j^k$\n",
    "2. Caculate the _backward phase_ from the final output layer-m to the input layer, and store the results $\\frac{\\partial E_d}{\\partial w_{ij}^k}$ ($w_{ij}^k$ is the weight connecting node-i in layer-(k-1) to node-j in layer-k)\n",
    "    - 2.1 - calcualte error term for the final layer-m: $\\delta_1^m = g_o'(a_1^m)(\\hat y_d - y_d)$ \n",
    "    - 2.2 - balckpropagate the error term for the hidden layers: $\\delta_j^k = g'(a_j^k)\\sum_{l=1}^{r^{k+1}}{w_{jl}^{k+1}\\delta_l^{k+1}}$\n",
    "    - 2.3 - calculate the partial derivatives wrt each weight: $w_{ij}^k = \\delta_j^ko_i^{k-1}$\n",
    "3. Combine the individual gradients for each input-output pair: $\\frac{\\partial E(X, \\theta)}{\\partial w_{ij}^k} = \\frac{1}{N}\\sum_{d=1}^N\\frac{\\partial E_d}{\\partial w_{ij}^k}$\n",
    "4. Update the weights: $\\Delta w_{ij}^k = -\\alpha \\frac{\\partial E(X, \\theta)}{\\partial w_{ij}^k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6476912306619782"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following example uses 1 hidden layer and 1 output node\n",
    "\n",
    "class simple_nn():\n",
    "    # initialize\n",
    "    def __init__(self, X, y, n_hidden=3, n_iter=10000, lr=0.1):\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_data, self.dim_inputs = X.shape\n",
    "        self.dim_outputs = y.shape[1]\n",
    "        self.y = y\n",
    "        self.X = np.hstack((np.ones((self.n_data, 1)), X))\n",
    "        self.hidden_weights = 2 * np.random.random((self.dim_inputs + 1, self.n_hidden)) - 1\n",
    "        self.output_weights = 2 * np.random.random((self.n_hidden + 1, self.dim_outputs)) - 1\n",
    "        \n",
    "    # define sigmoid function with derivative option\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative == True:\n",
    "            return x * (1-x)\n",
    "        else:\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def fit(self):\n",
    "        for i in range(self.n_iter):\n",
    "            # forward phase\n",
    "            hidden_layer_outputs = np.hstack((np.ones((self.n_data, 1)), self.sigmoid(self.X.dot(self.hidden_weights))))\n",
    "            output_layer_outputs = hidden_layer_outputs.dot(self.output_weights)\n",
    "            \n",
    "            # backward phase\n",
    "            # calculate output layer error term\n",
    "            output_error = output_layer_outputs - self.y\n",
    "            # calcualte hidden layer error term\n",
    "            hidden_error = hidden_layer_outputs[:, 1:] * (1 - hidden_layer_outputs[:, 1:]) * np.dot(output_error, self.output_weights.T[:, 1:])\n",
    "            \n",
    "            # partital derivatives\n",
    "            hidden_pd = self.X[:, :, np.newaxis] * hidden_error[:, np.newaxis, :]\n",
    "            output_pd = hidden_layer_outputs[:, :, np.newaxis] * output_error[:, np.newaxis, :]\n",
    "            \n",
    "            # average for total gradients\n",
    "            self.total_hidden_gradient = hidden_pd.mean(axis=0)\n",
    "            self.total_output_gradient = output_pd.mean(axis=0)\n",
    "            \n",
    "            # update weights\n",
    "            self.hidden_weights += -self.lr * self.total_hidden_gradient\n",
    "            self.output_weights += -self.lr * self.total_output_gradient\n",
    "            \n",
    "            if i % 500 == 0:\n",
    "                print(f'output after training iteration {i}: {output_layer_outputs}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake data\n",
    "X = np.array([  \n",
    "    [0, 0, 1],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 1, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1],\n",
    "])\n",
    "\n",
    "y = np.array([[0, 1, 0, 1, 1, 0]]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output after training iteration 0: [[0.56213115]\n",
      " [0.44555914]\n",
      " [0.47731602]\n",
      " [0.36361526]\n",
      " [0.47747825]\n",
      " [0.37241419]]\n",
      "output after training iteration 500: [[0.43679569]\n",
      " [0.67451019]\n",
      " [0.40351425]\n",
      " [0.65781425]\n",
      " [0.29035525]\n",
      " [0.52642169]]\n",
      "output after training iteration 1000: [[0.38967252]\n",
      " [0.7030734 ]\n",
      " [0.38370924]\n",
      " [0.71375792]\n",
      " [0.25690025]\n",
      " [0.54320504]]\n",
      "output after training iteration 1500: [[0.36743917]\n",
      " [0.72187583]\n",
      " [0.36704491]\n",
      " [0.73981058]\n",
      " [0.25157379]\n",
      " [0.54003994]]\n",
      "output after training iteration 2000: [[0.34198746]\n",
      " [0.75031232]\n",
      " [0.34136624]\n",
      " [0.77012587]\n",
      " [0.2522033 ]\n",
      " [0.52978184]]\n",
      "output after training iteration 2500: [[0.30715247]\n",
      " [0.79215255]\n",
      " [0.30458624]\n",
      " [0.80982721]\n",
      " [0.26193012]\n",
      " [0.51116203]]\n",
      "output after training iteration 3000: [[0.26891903]\n",
      " [0.83986972]\n",
      " [0.26459716]\n",
      " [0.85208211]\n",
      " [0.28447847]\n",
      " [0.4816553 ]]\n",
      "output after training iteration 3500: [[0.24038876]\n",
      " [0.87447131]\n",
      " [0.23593791]\n",
      " [0.88250689]\n",
      " [0.32695421]\n",
      " [0.4354427 ]]\n",
      "output after training iteration 4000: [[0.22930392]\n",
      " [0.89804033]\n",
      " [0.22836253]\n",
      " [0.90287485]\n",
      " [0.4056371 ]\n",
      " [0.3331587 ]]\n",
      "output after training iteration 4500: [[0.19582463]\n",
      " [0.92931143]\n",
      " [0.20316995]\n",
      " [0.92700518]\n",
      " [0.53037896]\n",
      " [0.21536165]]\n",
      "output after training iteration 5000: [[0.12904705]\n",
      " [0.95771394]\n",
      " [0.14281516]\n",
      " [0.94777194]\n",
      " [0.68772736]\n",
      " [0.13570799]]\n",
      "output after training iteration 5500: [[0.06802518]\n",
      " [0.97817762]\n",
      " [0.07909068]\n",
      " [0.96810311]\n",
      " [0.82921656]\n",
      " [0.07742125]]\n",
      "output after training iteration 6000: [[0.03129465]\n",
      " [0.98963323]\n",
      " [0.03675919]\n",
      " [0.98408103]\n",
      " [0.91964124]\n",
      " [0.03846522]]\n",
      "output after training iteration 6500: [[0.0134209 ]\n",
      " [0.99537998]\n",
      " [0.01566609]\n",
      " [0.99297278]\n",
      " [0.96524338]\n",
      " [0.0172294 ]]\n",
      "output after training iteration 7000: [[0.00556522]\n",
      " [0.99803883]\n",
      " [0.00645701]\n",
      " [0.99705843]\n",
      " [0.98554456]\n",
      " [0.00729293]]\n",
      "output after training iteration 7500: [[0.00227369]\n",
      " [0.9991897 ]\n",
      " [0.00262951]\n",
      " [0.99879421]\n",
      " [0.99408755]\n",
      " [0.00300651]]\n",
      "output after training iteration 8000: [[9.23172145e-04]\n",
      " [9.99669384e-01]\n",
      " [1.06609120e-03]\n",
      " [9.99509803e-01]\n",
      " [9.97598343e-01]\n",
      " [1.22534993e-03]]\n",
      "output after training iteration 8500: [[3.73878040e-04]\n",
      " [9.99865829e-01]\n",
      " [4.31494615e-04]\n",
      " [9.99801374e-01]\n",
      " [9.99027172e-01]\n",
      " [4.97033722e-04]]\n",
      "output after training iteration 9000: [[1.51261973e-04]\n",
      " [9.99945672e-01]\n",
      " [1.74528270e-04]\n",
      " [9.99919625e-01]\n",
      " [9.99606389e-01]\n",
      " [2.01215942e-04]]\n",
      "output after training iteration 9500: [[6.11714164e-05]\n",
      " [9.99978022e-01]\n",
      " [7.05732393e-05]\n",
      " [9.99967493e-01]\n",
      " [9.99840816e-01]\n",
      " [8.13942047e-05]]\n"
     ]
    }
   ],
   "source": [
    "model = simple_nn(X, y)\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
